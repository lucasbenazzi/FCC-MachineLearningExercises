{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasbenazzi/FCC-MachineLearningExercises/blob/main/fcc_sms_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
        "\n",
        "---\n",
        "\n",
        "In this challenge, you need to create a machine learning model that will classify SMS messages as either \"ham\" or \"spam\". A \"ham\" message is a normal message sent by a friend. A \"spam\" message is an advertisement or a message sent by a company.\n",
        "\n",
        "You should create a function called `predict_message` that takes a message string as an argument and returns a list. The first element in the list should be a number between zero and one that indicates the likeliness of \"ham\" (0) or \"spam\" (1). The second element in the list should be the word \"ham\" or \"spam\", depending on which is most likely.\n",
        "\n",
        "For this challenge, you will use the [SMS Spam Collection dataset](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/). The dataset has already been grouped into train data and test data.\n",
        "\n",
        "The first two cells import the libraries and data. The final cell tests your model and function. Add your code in between these cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a50734a-cc07-4470-bbe4-7862110de988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.62.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (21.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.1.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets) (3.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.54.0)\n",
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  #!pip install tf-nightly\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e3e836-aa66-4cc4-8e00-cb49816eb339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-26 13:39:03--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.3.33, 104.26.2.33, 172.67.70.149, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.3.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358233 (350K) [text/tab-separated-values]\n",
            "Saving to: ‘train-data.tsv.1’\n",
            "\n",
            "\rtrain-data.tsv.1      0%[                    ]       0  --.-KB/s               \rtrain-data.tsv.1    100%[===================>] 349.84K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-01-26 13:39:03 (105 MB/s) - ‘train-data.tsv.1’ saved [358233/358233]\n",
            "\n",
            "--2022-01-26 13:39:04--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.3.33, 104.26.2.33, 172.67.70.149, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.3.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118774 (116K) [text/tab-separated-values]\n",
            "Saving to: ‘valid-data.tsv.1’\n",
            "\n",
            "valid-data.tsv.1    100%[===================>] 115.99K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-01-26 13:39:04 (55.3 MB/s) - ‘valid-data.tsv.1’ saved [118774/118774]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra imports\n",
        "from keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "Q2b73TbyBc7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  create dataframes from files\n",
        "df_train = pd.read_csv(train_file_path, sep='\\t', header = None, names = ['status','text'])\n",
        "df_test = pd.read_csv(test_file_path, sep='\\t', header = None, names = ['status','text'])"
      ],
      "metadata": {
        "id": "vYv8Nbj4lXFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to define a set lenght for each input\n",
        "\n",
        "# create a series with the lenght of words on each text\n",
        "# this steps also normalizes the words the same way will normalize later\n",
        "aux_df = df_train['text'].copy().\\\n",
        "str.replace('[^a-zA-Z]', ' ').\\\n",
        "str.replace('\\s+', ' ', regex=True).\\\n",
        "str.lower().\\\n",
        "str.split().\\\n",
        "str.len()\n",
        "\n",
        "print(aux_df.describe())\n",
        "\n",
        "# we can see that most texts have fewer then 23 words\n",
        "MAX_LENGTH = int(aux_df.quantile(0.75))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQL5G1nZKxCV",
        "outputId": "bc0cbe24-07e2-4b9a-fda2-8e0e98ec60cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    4179.000000\n",
            "mean       15.708064\n",
            "std        11.519769\n",
            "min         0.000000\n",
            "25%         7.000000\n",
            "50%        12.000000\n",
            "75%        23.000000\n",
            "max       190.000000\n",
            "Name: text, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since we need numbers as inputs, create a word_map variable to map every word in the training set to a number\n",
        "def create_word_map(dataframe):\n",
        "  df = dataframe.copy()\n",
        "  \n",
        "  # only leave alphabetical values, and make all words lower case\n",
        "  df['text'] = df['text'].str.replace('[^a-zA-Z]', ' ').str.replace('\\s+', ' ', regex=True).str.lower()\n",
        "\n",
        "  # create a dataframe with the value count of each word, so words that appear more often have a lower number attributed to it\n",
        "  word_map  = df['text'].str.split(expand=True).stack().value_counts().reset_index().rename(columns = {'index':'word', 0:'count'})\n",
        "\n",
        "  # disregard words that appear only once\n",
        "  word_map =  word_map.loc[word_map['count'] > 1]\n",
        "\n",
        "  # create a new column with index value + 1, so the most common word is 1 instead of 0\n",
        "  word_map['id'] = word_map.index + 1\n",
        "\n",
        "  return word_map[['word','id']].set_index('word')"
      ],
      "metadata": {
        "id": "__DzSWDroMX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a \"values\" and a \"labels\" variables based on the \"word_map\" variable\n",
        "def prepare_numpy_array(dataframe, word_map):\n",
        "  df = dataframe.copy()\n",
        "\n",
        "  # convert 'status' column to numbers\n",
        "  df['status'] = df['status'].replace({\n",
        "          'ham':0,'spam':1})\n",
        "  \n",
        "  # only leave alphabetical values, and make all words lower case\n",
        "  df['text'] = df['text'].str.replace('[^a-zA-Z]', ' ').str.replace('\\s+', ' ', regex=True).str.lower()\n",
        "\n",
        "  # create a numby array for both \"text\" column and \"status\"\n",
        "  text_array = df['text'].to_numpy()\n",
        "\n",
        "  # create a new list where each word is converted to it's corresponding number\n",
        "  # this method ussed instead of the \"pandas.apply\" function to save processing time\n",
        "  value_dataset = []\n",
        "  for index in range(text_array.shape[0]):\n",
        "    text_list = text_array[index].split()\n",
        "    number_list = []\n",
        "\n",
        "    for word in text_list:\n",
        "      # if the word isn't present in the word map variable, disregard it\n",
        "      if word in word_map.index:\n",
        "        number_list.append(word_map.loc[word][0])\n",
        "\n",
        "    value_dataset.append(number_list)\n",
        "\n",
        "  # add leading zeros or delete extra words in order to standardize the length according to the MAX_LENGTH variable\n",
        "  value_dataset = sequence.pad_sequences(value_dataset, MAX_LENGTH)\n",
        "\n",
        "  return np.array(value_dataset), np.array(df['status'])"
      ],
      "metadata": {
        "id": "vffssVuZjGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_map = create_word_map(df_train)\n",
        "\n",
        "train_data, train_labels = prepare_numpy_array(df_train, word_map)"
      ],
      "metadata": {
        "id": "kGpyf41JvLR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add 1 to the vocabulary size to account empty (zero value) elements\n",
        "VOCAB_SIZE = word_map.shape[0] + 1\n",
        "\n",
        "# create, compile and train the model\n",
        "# for this classification problem will first ad a word embedding layer followed by a lstm layer\n",
        "# since it's a binary classification problem, the last layer will have one neuron and have a sigmoid activation function\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['acc']\n",
        "    )\n",
        "\n",
        "# preparting the test dataframe to use as validation\n",
        "test_data, test_labels = prepare_numpy_array(df_test, word_map)\n",
        "\n",
        "model.fit(train_data, train_labels, epochs=10, validation_data = (test_data, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lERLPWuxPApE",
        "outputId": "25332052-5f8f-4d3c-89c2-060124a654f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "131/131 [==============================] - 5s 19ms/step - loss: 0.2553 - acc: 0.9179 - val_loss: 0.0916 - val_acc: 0.9792\n",
            "Epoch 2/10\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0528 - acc: 0.9885 - val_loss: 0.0429 - val_acc: 0.9871\n",
            "Epoch 3/10\n",
            "131/131 [==============================] - 2s 16ms/step - loss: 0.0259 - acc: 0.9935 - val_loss: 0.0455 - val_acc: 0.9835\n",
            "Epoch 4/10\n",
            "131/131 [==============================] - 2s 16ms/step - loss: 0.0160 - acc: 0.9962 - val_loss: 0.0397 - val_acc: 0.9892\n",
            "Epoch 5/10\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0090 - acc: 0.9981 - val_loss: 0.0404 - val_acc: 0.9885\n",
            "Epoch 6/10\n",
            "131/131 [==============================] - 2s 16ms/step - loss: 0.0057 - acc: 0.9986 - val_loss: 0.0376 - val_acc: 0.9899\n",
            "Epoch 7/10\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0367 - val_acc: 0.9914\n",
            "Epoch 8/10\n",
            "131/131 [==============================] - 2s 16ms/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0514 - val_acc: 0.9892\n",
            "Epoch 9/10\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0549 - val_acc: 0.9864\n",
            "Epoch 10/10\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0542 - val_acc: 0.9892\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ea92d2e90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab4da43-9f52-4c2f-f938-f4cb06d38483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7.8286655e-05, 'ham']\n"
          ]
        }
      ],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "  # transform the string into an array with the same rules applied to the training data\n",
        "\n",
        "  # convert text into a dataframe, to take advantage of the already created functions \"prepare_numpy_array\"\n",
        "  pred_text_df = pd.DataFrame(data = [['ham', pred_text]], columns = ['status','text'])\n",
        "\n",
        "  pred_text, _ = prepare_numpy_array(pred_text_df, word_map)\n",
        "\n",
        "  # make the prediction\n",
        "  prediction = model.predict(pred_text)[0][0]\n",
        "  \n",
        "  if prediction >= 0.5:\n",
        "    prediction_status = 'spam'\n",
        "  else:\n",
        "    prediction_status = 'ham'\n",
        "\n",
        "  return [prediction, prediction_status]\n",
        "\n",
        "pred_text = \"how are you doing today?\"\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da02993-72ad-431e-f723-396d26328984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You passed the challenge. Great job!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sources\n",
        "\n",
        "1. \"pandas.read_csv\" pandas, https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "2. \"f.keras.preprocessing.sequence.pad_sequences\" TensorFlow, https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "3. \"How to Make Predictions with Long Short-Term Memory Models in Keras\" Machine Learning Mastery, https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/\n",
        "4. \"6 Practices to enhance the performance of a Text Classification Model\" Analytics Vidhya, https://www.analyticsvidhya.com/blog/2015/10/6-practices-enhance-performance-text-classification-model/\n",
        "\n",
        "\n",
        "##Extra links\n",
        "\n",
        "This project was also based in part on the Google colab file from FreeCodeCamp \"Machine Learning with Python\" Course.\n",
        "\n",
        "https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true&scrollTo=OWGGcBIpjrMu\n",
        "\n",
        "###The following Stackoverflow links where used to help on this project\n",
        "\n",
        "1. https://stackoverflow.com/questions/9652832/how-to-load-a-tsv-file-into-a-pandas-dataframe\n",
        "2. https://stackoverflow.com/questions/46241120/how-to-remove-non-alpha-numeric-characters-from-strings-within-a-dataframe-colum\n",
        "3. https://stackoverflow.com/questions/43071415/remove-multiple-blanks-in-dataframe\n",
        "4. https://stackoverflow.com/questions/46786211/counting-the-frequency-of-words-in-a-pandas-data-frame"
      ],
      "metadata": {
        "id": "PobVlUTlmj_U"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "fcc_sms_text_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}